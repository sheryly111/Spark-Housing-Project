{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7dbf56",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95c1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shery\\AppData\\Local\\Temp\\ipykernel_18816\\2723244342.py:5: DtypeWarning: Columns (1,7,22,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data = pd.read_csv('raw_merged.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv('raw_merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8199ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (2164743, 32)\n",
      "Validation data shape: (463874, 32)\n",
      "Test data shape: (463874, 32)\n"
     ]
    }
   ],
   "source": [
    "raw_data = raw_data.replace(['EX - Excellent'] , 'E - Excellent')\n",
    "raw_data[\"case_created_date\"] = raw_data[\"case_created_date\"][:11]\n",
    "raw_data[\"close_date\"] = raw_data[\"close_date\"][:11]\n",
    "raw_data[\"last_case_update\"] = raw_data[\"last_case_update\"][:11]\n",
    "\n",
    "#split data insto 70 train, 15 val, 15 test\n",
    "train_data, temp_data = train_test_split(raw_data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c061568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_cond\n",
      "A - Average       1798955\n",
      "G - Good           111181\n",
      "E - Excellent       30844\n",
      "VG - Very Good       7101\n",
      "F - Fair              689\n",
      "P - Poor               12\n",
      "Name: count, dtype: int64\n",
      "NA COUNTS:  Unnamed: 0                 0\n",
      "over_5               1976368\n",
      "year                       0\n",
      "full_address               0\n",
      "sam_id                     0\n",
      "building_id                0\n",
      "ward_id                    0\n",
      "parcel_num                 0\n",
      "case_created_date    2164735\n",
      "last_case_update     2164735\n",
      "case_enquiry_id        11893\n",
      "targeted_deadline     303630\n",
      "close_date           2164737\n",
      "case_met_deadline      11900\n",
      "case_status            11893\n",
      "closure_reason         11893\n",
      "case_title             11893\n",
      "case_subject           11893\n",
      "case_reason            11893\n",
      "case_type              11893\n",
      "case_department        11893\n",
      "p_id                  191098\n",
      "cm_id                 247421\n",
      "gis_id                191320\n",
      "landlord_name         191168\n",
      "int_cond              608429\n",
      "ext_cond              568442\n",
      "overall_cond          215961\n",
      "bdrm_cond             634892\n",
      "heat_type             608428\n",
      "ac_type               608427\n",
      "num_bed_rms           581337\n",
      "dtype: int64\n",
      "num sam ids:  8050\n"
     ]
    }
   ],
   "source": [
    "print(train_data['overall_cond'].value_counts())\n",
    "print('NA COUNTS: ', train_data.isna().sum())\n",
    "print('num sam ids: ', train_data['sam_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91758658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411370     A - Average\n",
       "1985085    A - Average\n",
       "1482786    A - Average\n",
       "2112011            NaN\n",
       "495677     A - Average\n",
       "Name: overall_cond, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n",
    "train_data['overall_cond'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70189115",
   "metadata": {},
   "source": [
    "# Additional Column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2b8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_cond\n",
      "A - Average       1798955\n",
      "G - Good           111181\n",
      "E - Excellent       30844\n",
      "VG - Very Good       7101\n",
      "F - Fair              689\n",
      "P - Poor               12\n",
      "Name: count, dtype: int64\n",
      "prob_address\n",
      "0    2164042\n",
      "1        701\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check(cond):\n",
    "        if cond == 'F - Fair' or cond == 'P - Poor':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "train_data['prob_address'] = train_data['overall_cond'].apply(check)\n",
    "\n",
    "\n",
    "print(train_data['overall_cond'].value_counts())\n",
    "print(train_data['prob_address'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3008bd",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c65077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking smaller sample size for faster processing...\n",
      "Sampled training set shape:  (432949, 33)\n",
      "Sampled score distribution prob_address\n",
      "0    432809\n",
      "1       140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sample_size = 0.2\n",
    "print(\"Taking smaller sample size for faster processing...\")\n",
    "\n",
    "train_data_with_target = train_data[train_data[\"prob_address\"].notna()].copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, train_data_sample = train_test_split(\n",
    "    train_data_with_target,\n",
    "    test_size = sample_size, \n",
    "    random_state=42, \n",
    "    stratify=train_data_with_target[\"prob_address\"] \n",
    ")\n",
    "\n",
    "print(\"Sampled training set shape: \", train_data_sample.shape)\n",
    "print(\"Sampled score distribution\", train_data_sample[\"prob_address\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47a8da",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e491b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime \n",
    "from datetime import date\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "import pickle \n",
    "\n",
    "#feature engineering function\n",
    "def create_features(dataset):\n",
    "\n",
    "    #Feature 0: Fix Values \n",
    "    \n",
    "    # Fill in Nans \n",
    "    print(\"filling in nans...\")\n",
    "    dataset[\"case_title\"] = dataset[\"case_title\"].fillna(\"\").astype(str)\n",
    "    dataset[\"case_subject\"] = dataset[\"case_subject\"].fillna(\"\").astype(str)\n",
    "    dataset[\"case_reason\"] = dataset[\"case_reason\"].fillna(\"\").astype(str)\n",
    "    dataset[\"over_5\"] = dataset[\"over_5\"].fillna(\"\").astype(str)\n",
    "    dataset[\"int_cond\"] = dataset[\"int_cond\"].fillna(\"\").astype(str)\n",
    "    dataset[\"ext_cond\"] = dataset[\"ext_cond\"].fillna(\"\").astype(str)\n",
    "    dataset[\"overall_cond\"] = dataset[\"overall_cond\"].fillna(\"\").astype(str)\n",
    "    dataset[\"bdrm_cond\"] = dataset[\"bdrm_cond\"].fillna(\"\").astype(str)\n",
    "    dataset[\"heat_type\"] = dataset[\"heat_type\"].fillna(\"\").astype(str)\n",
    "    dataset[\"ac_type\"] = dataset[\"ac_type\"].fillna(\"\").astype(str)\n",
    "    dataset[\"parcel_num\"] = dataset[\"parcel_num\"].astype(str)\n",
    "    dataset[\"cm_id\"] = dataset[\"cm_id\"].astype(str)\n",
    "\n",
    "    dataset[\"close_date\"] = dataset[\"close_date\"].fillna(date.today())\n",
    "    dataset[\"close_date\"] = dataset[\"close_date\"].astype(str)\n",
    "    \n",
    "    # String -> Integer \n",
    "    dataset[\"year\"] = dataset[\"year\"].astype(int)\n",
    "\n",
    "    # Standardize Values\n",
    "    dataset[\"over_5\"] = dataset[\"over_5\"].replace([\"N\", \"No\"], \"NO\")\n",
    "    dataset[\"over_5\"] = dataset[\"over_5\"].replace([\"Y\"], \"YES\")\n",
    "    dataset[\"over_5\"] = dataset[\"over_5\"].replace([\"nan\"], \"\")\n",
    "    \n",
    "    #Feature 1: Text Features \n",
    "    print(\"Extracting text features...\")\n",
    "    dataset[\"case_reason_length\"] = dataset[\"case_reason\"].fillna('').astype(str).str.len()\n",
    "    dataset['case_reason_word_count'] = dataset['case_reason'].fillna('').astype(str).str.split().str.len()\n",
    "\n",
    "    dataset['case_title_length'] = dataset['case_title'].fillna('').astype(str).str.len()\n",
    "    dataset['case_title_word_count'] = dataset['case_title'].fillna('').astype(str).str.split().str.len()\n",
    "\n",
    "    dataset['case_type_length'] = dataset['case_type'].fillna('').astype(str).str.len()\n",
    "    dataset['case_type_word_count'] = dataset['case_type'].fillna('').astype(str).str.split().str.len()\n",
    "\n",
    "    #Feature 2: Time Based Features \n",
    "    print(\"doing time based features...\")\n",
    "    dataset['case_date'] = pd.to_datetime(dataset['case_created_date'], unit='s')\n",
    "    dataset['case_year'] = dataset['case_date'].dt.year\n",
    "    dataset['case_month'] = dataset['case_date'].dt.month\n",
    "    dataset['case_day_of_week'] = dataset['case_date'].dt.dayofweek\n",
    "\n",
    "    dataset['case_closing_date'] = pd.to_datetime(dataset['close_date'])\n",
    "    dataset['close_year'] = dataset['case_closing_date'].dt.year\n",
    "    dataset['close_month'] = dataset['case_closing_date'].dt.month\n",
    "    dataset['close_day_of_week'] = dataset['case_closing_date'].dt.dayofweek\n",
    "\n",
    "\n",
    "    # Feature 3: Cyclical encoding for month and day of week\n",
    "    print(\"Cyclical encoding for month and day...\")\n",
    "    dataset['created_month_sin'] = np.sin(2 * np.pi * dataset['case_month'] / 12)\n",
    "    dataset['created_month_cos'] = np.cos(2 * np.pi * dataset['case_month'] / 12)\n",
    "    dataset['created_dow_sin'] = np.sin(2 * np.pi * dataset['case_day_of_week'] / 7)\n",
    "    dataset['created_dow_cos'] = np.cos(2 * np.pi * dataset['case_day_of_week'] / 7)\n",
    "\n",
    "    dataset['closed_month_sin'] = np.sin(2 * np.pi * dataset['close_month'] / 12)\n",
    "    dataset['closed_month_cos'] = np.cos(2 * np.pi * dataset['close_month'] / 12)\n",
    "    dataset['closed_dow_sin'] = np.sin(2 * np.pi * dataset['close_day_of_week'] / 7)\n",
    "    dataset['closed_dow_cos'] = np.cos(2 * np.pi * dataset['close_day_of_week'] / 7)\n",
    "\n",
    "    # Feature 3.1: Length of case open\n",
    "    dataset['case_open_days'] = (dataset['case_closing_date'] - dataset['case_date']).dt.days\n",
    "    dataset[\"avg_resolution_time\"] = dataset.groupby(\"full_address\")[\"case_open_days\"].transform(\"mean\")\n",
    "\n",
    "    # Feature 3.2: Num cases per address\n",
    "    print(\"calculating cases per address per year...\")\n",
    "    cases_per_address_year = (\n",
    "        dataset.groupby([\"full_address\", \"year\"])[\"case_enquiry_id\"]\n",
    "            .nunique()\n",
    "            .reset_index(name=\"case_count\")\n",
    "        )\n",
    "\n",
    "    dataset = dataset.merge(  # your base dataset of all addresses\n",
    "        cases_per_address_year,\n",
    "        on=[\"full_address\", \"year\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Feature 4: Severity Analysis\n",
    "    print(\"performing sentiment intensity analysis...\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataset[\"case_reason_severity_score\"] = dataset[\"case_reason\"].apply(lambda x: sia.polarity_scores(str(x))[\"compound\"])\n",
    "    dataset[\"case_title_severity_score\"] = dataset[\"case_title\"].apply(lambda x: sia.polarity_scores(str(x))[\"compound\"])\n",
    "    \n",
    "    # Feature 5: TF-IDF\n",
    "    print(\"doing tfidf on title and case reasons...\")\n",
    "    # For case reason: \n",
    "    case_reason_tfidf = TfidfVectorizer(\n",
    "        max_features=2000,    # adjust based on memory / dataset size --> 4k \n",
    "        ngram_range=(1, 2),   # unigrams + bigrams\n",
    "        stop_words='english',\n",
    "        min_df=10,             \n",
    "        max_df=0.8,\n",
    "        sublinear_tf=True           \n",
    "    )\n",
    "    \n",
    "    case_reason_tfidf_transformed = case_reason_tfidf.fit_transform(dataset['case_reason']) \n",
    "\n",
    "    # For case title \n",
    "    case_title_tfidf = TfidfVectorizer(\n",
    "        max_features=3000,    # adjust based on memory / dataset size --> 4k \n",
    "        ngram_range=(1, 2),   # unigrams + bigrams\n",
    "        stop_words='english',\n",
    "        min_df=10,             \n",
    "        max_df=0.8,\n",
    "        sublinear_tf=True           \n",
    "    )\n",
    "    \n",
    "    case_title_tfidf_transformed = case_title_tfidf.fit_transform(dataset['case_title']) \n",
    "    with open ('case_reason_tfidf.pkl', 'wb') as f:\n",
    "        pickle.dump(case_reason_tfidf, f)\n",
    "    with open ('case_title_tfidf.pkl', 'wb') as f:\n",
    "        pickle.dump(case_title_tfidf, f)\n",
    "\n",
    "    # Feature 5.1: SVD \n",
    "    print(\"truncate svd-ing...\")\n",
    "    svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "    case_reason_svd = svd.fit_transform(case_reason_tfidf_transformed)\n",
    "    case_title_svd = svd.fit_transform(case_title_tfidf_transformed)\n",
    "\n",
    "    for i in range(case_reason_svd.shape[1]):\n",
    "        dataset[f'reason_tfidf_svd{i}'] = case_reason_svd[:,i]\n",
    "\n",
    "    for i in range(case_title_svd.shape[1]):\n",
    "        dataset[f'title_tfidf_svd{i}'] = case_title_svd[:,i]\n",
    "\n",
    "    with open ('case_reason_svd.pkl', 'wb') as f:\n",
    "        pickle.dump(case_reason_svd, f)\n",
    "    with open ('case_title_svd.pkl', 'wb') as f:\n",
    "        pickle.dump(case_title_svd, f)\n",
    "        \n",
    "\n",
    "    # Feature 6: Frequency of Cases\n",
    "    print(\"finding frequency of cases...\")\n",
    "    dataset[\"years_with_cases\"] = dataset.groupby(\"full_address\")[\"year\"].transform(\"nunique\")\n",
    "    dataset[\"total_case_count_address\"] = dataset.groupby(\"full_address\")[\"case_enquiry_id\"].transform(\"count\")\n",
    "    dataset[\"avg_cases_per_year\"] = dataset[\"total_case_count_address\"] / dataset[\"years_with_cases\"]\n",
    "\n",
    "    # Feature 7: Escalation of Severity\n",
    "    print(\"finding escalation of severity... \")\n",
    "    '''\n",
    "    year_counts = dataset.groupby([\"full_address\", \"year\"])[\"case_enquiry_id\"].count()\n",
    "    dataset[\"year_over_year_increase\"] = year_counts.groupby(level=0).diff().fillna(0)\n",
    "    ''' \n",
    "    # Feature 7: Escalation of Severity\n",
    "    print(\"finding escalation of severity... \")\n",
    "    counts_df = (\n",
    "        dataset.groupby([\"full_address\", \"year\"], as_index=False)[\"case_enquiry_id\"]\n",
    "            .count()\n",
    "            .rename(columns={\"case_enquiry_id\": \"case_count\"})\n",
    "    )\n",
    "    counts_df = counts_df.sort_values([\"full_address\", \"year\"])\n",
    "    counts_df[\"year_over_year_increase\"] = counts_df.groupby(\"full_address\")[\"case_count\"].diff().fillna(0)\n",
    "\n",
    "    # Merge results back to dataset\n",
    "    dataset = dataset.merge(counts_df[[\"full_address\", \"year\", \"case_count\", \"year_over_year_increase\"]],\n",
    "                            on=[\"full_address\", \"year\"],\n",
    "                            how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "    # Feature 8: Repeat closure failure / missed deadlines\n",
    "    print(\"finding if metdeadline...\")\n",
    "    dataset['met_deadline'] = (dataset['case_met_deadline'] == \"YES\").astype(int)\n",
    "    dataset['deadline_fail_rate'] = dataset.groupby(\"full_address\")[\"met_deadline\"].transform(lambda x: 1 - x.mean())\n",
    "\n",
    "    # Feature 9: Condition Flags\n",
    "    print(\"mapping conditions to scores \")\n",
    "    condition_map = {\n",
    "        \"E - Excellent\" : 1,\n",
    "        \"VG - Very Good\" : 2,\n",
    "        \"G - Good\" : 3,  \n",
    "        \"A - Average\": 4,\n",
    "        \"F - Fair\" : 5, \n",
    "        \"P - Poor\" : 6  \n",
    "    }\n",
    "\n",
    "    dataset[\"overall_cond_score\"] = dataset[\"overall_cond\"].map(condition_map).fillna(0)\n",
    "    dataset[\"int_cond_score\"] = dataset[\"int_cond\"].map(condition_map).fillna(0)\n",
    "    dataset[\"ext_cond_score\"] = dataset[\"ext_cond\"].map(condition_map).fillna(0)\n",
    "\n",
    "    dataset[\"avg_overall_condition\"] = dataset.groupby(\"full_address\")[\"overall_cond_score\"].transform(\"mean\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0f197",
   "metadata": {},
   "source": [
    "# Apply Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b25f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling in nans...\n",
      "Extracting text features...\n",
      "doing time based features...\n",
      "Cyclical encoding for month and day...\n",
      "calculating cases per address per year...\n",
      "performing sentiment intensity analysis...\n",
      "doing tfidf on title and case reasons...\n",
      "truncate svd-ing...\n",
      "finding frequency of cases...\n",
      "finding escalation of severity... \n",
      "finding escalation of severity... \n",
      "finding if metdeadline...\n",
      "mapping conditions to scores \n"
     ]
    }
   ],
   "source": [
    "#apply feature engineering \n",
    "\n",
    "dataset = train_data_sample.copy() \n",
    "\n",
    "dataset_features = create_features(dataset) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bc2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
